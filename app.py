import os
import re
import requests
import gradio as gr
from openai import OpenAI
from PyPDF2 import PdfReader
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from duckduckgo_search import DDGS

load_dotenv()
api_key = os.getenv("API_KEY")
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=api_key
)

def is_url(message):
    """Check if the message is a URL."""
    
    return re.match(r'https?://\S+', message)

def extract_text_from_pdf(file):
    """Extract text from a PDF file."""

    try:
        reader = PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        return f"L·ªói khi ƒë·ªçc PDF: {str(e)}"

def summarize_url(url):
    """Summarize the content of a URL."""

    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        text = " ".join([p.get_text() for p in soup.find_all("p")])[:4000]
        messages = [
            {"role": "system", "content": "T√≥m t·∫Øt n·ªôi dung b√†i b√°o sau b·∫±ng ti·∫øng Vi·ªát:"},
            {"role": "user", "content": text}
        ]
        completion = client.chat.completions.create(
            model="meta-llama/llama-4-maverick",
            messages=messages
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"L·ªói khi truy c·∫≠p URL: {str(e)}"

def search_latest_event(query):
    """Search for the latest events or news related to the query."""

    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, region='wt-wt', safesearch='Moderate', max_results=3))
            if not results:
                return "Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p."

            content = "\n\n".join([f"{r['title']}\n{r['href']}\n{r['body']}" for r in results])
            messages = [
                {"role": "system", "content": "T√≥m t·∫Øt c√°c th√¥ng tin sau b·∫±ng ti·∫øng Vi·ªát:"},
                {"role": "user", "content": content}
            ]
            completion = client.chat.completions.create(
                model="meta-llama/llama-4-maverick",
                messages=messages
            )
            return completion.choices[0].message.content.strip()
    except Exception as e:
        return f"L·ªói khi t√¨m ki·∫øm: {str(e)}"

def ask_about_pdf(pdf_text, question):
    """Ask a question about the content of a PDF."""

    messages = [
        {"role": "system", "content": "B·∫°n l√† tr·ª£ l√Ω AI gi√∫p tr·∫£ l·ªùi c√°c c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung file PDF."},
        {"role": "user", "content": f"N·ªôi dung PDF:\n{pdf_text}"},
        {"role": "user", "content": f"C√¢u h·ªèi: {question}"}
    ]
    completion = client.chat.completions.create(
        model="meta-llama/llama-4-maverick",
        messages=messages
    )
    return completion.choices[0].message.content.strip()

def chat_with_bot(message, history, pdf_file=None):
    """Handle the chat interaction with the bot."""

    if not history:
        history = []

    if is_url(message):
        summary = summarize_url(message)
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": summary})
        yield history
        return

    if any(kw in message.lower() for kw in ["s·ª± ki·ªán", "tin t·ª©c", "m·ªõi nh·∫•t", "news"]):
        search_result = search_latest_event(message)
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": search_result})
        yield history
        return

    if pdf_file:
        text_from_pdf = extract_text_from_pdf(pdf_file)
        answer = ask_about_pdf(text_from_pdf, message)
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": answer})
        yield history
        return

    temp_history = history + [{"role": "user", "content": message}]
    stream = client.chat.completions.create(
        model="meta-llama/llama-4-maverick",
        messages=temp_history,
        stream=True
    )

    bot_response = ""
    for chunk in stream:
        if chunk.choices[0].delta.content is not None:
            bot_response += chunk.choices[0].delta.content
            yield history + [
                {"role": "user", "content": message},
                {"role": "assistant", "content": bot_response}
            ]

    history.append({"role": "user", "content": message})
    history.append({"role": "assistant", "content": bot_response})
    yield history

with gr.Blocks(theme=gr.themes.Soft()) as app:
    gr.Markdown("## ü§ñ Chat PDF + URL + Tin t·ª©c ‚Äî by AI Assistant")

    chatbot = gr.Chatbot( type='messages', height=450)
    pdf_file = gr.File(label="üìÑ T√†i li·ªáu PDF", file_types=[".pdf"])
    user_input = gr.Textbox(
        show_label=False,
        placeholder="Nh·∫≠p c√¢u h·ªèi, URL ho·∫∑c n·ªôi dung b·∫°n mu·ªën h·ªèi...",
        lines=1,
        autofocus=True
    )

    def submit_message(message, history, pdf_file):
        history = history or []
        for updated_history in chat_with_bot(message, history, pdf_file):
            yield "", updated_history

    user_input.submit(submit_message, [user_input, chatbot, pdf_file], [user_input, chatbot])


if __name__ == "__main__":
    app.launch()
